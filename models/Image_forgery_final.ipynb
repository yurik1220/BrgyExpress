{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7237acd3-be79-4d8c-8b7b-4b3b1b4caf6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x21c03cc1510>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# PIL/ELA\n",
    "from PIL import Image, ImageChops, ImageEnhance\n",
    "\n",
    "# Torch & vision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# Metrics & utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(2)\n",
    "torch.manual_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d817f3f8-a21b-4615-85a2-db4ba22fa9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ela_image(path, quality):\n",
    "    filename = path\n",
    "    im = Image.open(filename).convert('RGB')\n",
    "    # Save to in-memory buffer to avoid disk I/O stalls\n",
    "    buf = io.BytesIO()\n",
    "    im.save(buf, format='JPEG', quality=quality)\n",
    "    buf.seek(0)\n",
    "    resaved_im = Image.open(buf)\n",
    "\n",
    "    ela_im = ImageChops.difference(im, resaved_im)\n",
    "    extrema = ela_im.getextrema()\n",
    "    max_diff = max([ex[1] for ex in extrema])\n",
    "    if max_diff == 0:\n",
    "        max_diff = 1\n",
    "    scale = 255.0 / max_diff\n",
    "    ela_im = ImageEnhance.Brightness(ela_im).enhance(scale)\n",
    "\n",
    "    im.close()\n",
    "    resaved_im.close()\n",
    "    buf.close()\n",
    "    return ela_im\n",
    "\n",
    "\n",
    "def build_image_list(path_to_image, label, images):\n",
    "    for file in tqdm(os.listdir(path_to_image)):\n",
    "        try:\n",
    "            if file.endswith('jpg') or file.endswith('JPG') or file.endswith('jpeg') or file.endswith('JPEG'):\n",
    "                if int(os.stat(os.path.join(path_to_image, file)).st_size) > 10000:\n",
    "                    line = os.path.join(path_to_image, file) + ',' + label + '\\n'\n",
    "                    images.append(line)\n",
    "        except Exception:\n",
    "            print(os.path.join(path_to_image, file))\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76fc0fec-7d22-46a7-a72f-a463008f454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_path_original = r\"C:\\Users\\Julian\\Documents\\Dataset\\train\\authentic\"\n",
    "custom_path_tampered = r\"C:\\Users\\Julian\\Documents\\Dataset\\train\\tampered\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddb163ec-f9bd-444e-881e-154947a030aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_set = 'dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f951d191-ce50-4c99-be58-2b81b028ea45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 2100/2100 [00:00<00:00, 12549.35it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2100/2100 [00:00<00:00, 12203.18it/s]\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "images = build_image_list(custom_path_original, '0', images)\n",
    "images = build_image_list(custom_path_tampered, '1', images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c9b2ded-9ae6-4d99-97ef-f7cc4f0e4977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 4200/4200 [00:00<00:00, 1018152.63it/s]\n"
     ]
    }
   ],
   "source": [
    "image_name = []\n",
    "label = []\n",
    "for i in tqdm(range(len(images))):\n",
    "    image_name.append(images[i][0:-3])\n",
    "    label.append(images[i][-2])\n",
    "\n",
    "dataset = pd.DataFrame({'image':image_name,'class_label':label})\n",
    "dataset.to_csv(training_data_set,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0c58813-d0a8-4a55-8ce3-4f7571420390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have separate directories for validation/testing, set them here.\n",
    "# Expected structure (case-insensitive file suffixes jpg/jpeg):\n",
    "#   <root>/original/*.jpg\n",
    "#   <root>/forged/*.jpg\n",
    "val_root = r'C:\\Users\\Julian\\Documents\\Dataset\\validation'\n",
    "test_root = r'C:\\Users\\Julian\\Documents\\Dataset\\testing'\n",
    "\n",
    "class ELAImageFolder(Dataset):\n",
    "    def __init__(self, root_dir, transform, ela_quality=90):\n",
    "        self.root = root_dir\n",
    "        self.transform = transform\n",
    "        self.ela_quality = ela_quality\n",
    "        self.samples = []\n",
    "        # label map: original=0, forged=1\n",
    "        for label_name, label in [('original', 0), ('forged', 1)]:\n",
    "            folder = os.path.join(root_dir, label_name)\n",
    "            if not os.path.isdir(folder):\n",
    "                continue\n",
    "            for fn in os.listdir(folder):\n",
    "                if fn.lower().endswith(('jpg','jpeg')):\n",
    "                    full = os.path.join(folder, fn)\n",
    "                    if os.path.isfile(full) and os.stat(full).st_size > 10000:\n",
    "                        self.samples.append((full, float(label)))\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        ela_im = convert_to_ela_image(path, self.ela_quality)\n",
    "        img = self.transform(ela_im)\n",
    "        y = torch.tensor([label], dtype=torch.float32)\n",
    "        return img, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "094b381c-a393-4420-a551-95bb7154b202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using external validation from: C:\\Users\\Julian\\Documents\\Dataset\\validation (n=0)\n",
      "Using external testing from: C:\\Users\\Julian\\Documents\\Dataset\\testing (n=0)\n"
     ]
    }
   ],
   "source": [
    "# Switch validation loader to use external directory if present\n",
    "use_external_val = os.path.isdir(val_root)\n",
    "use_external_test = os.path.isdir(test_root)\n",
    "\n",
    "if use_external_val:\n",
    "    external_val_ds = ELAImageFolder(val_root, transform=val_transform, ela_quality=90)\n",
    "    val_loader = DataLoader(external_val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=torch.cuda.is_available())\n",
    "    print(f\"Using external validation from: {val_root} (n={len(external_val_ds)})\")\n",
    "else:\n",
    "    print(\"Using 20% split from dataset.csv for validation.\")\n",
    "\n",
    "if use_external_test:\n",
    "    test_ds = ELAImageFolder(test_root, transform=val_transform, ela_quality=90)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=torch.cuda.is_available())\n",
    "    print(f\"Using external testing from: {test_root} (n={len(test_ds)})\")\n",
    "else:\n",
    "    test_loader = None\n",
    "    print(\"No external testing directory found; skipping test evaluation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a90080c4-a07e-4407-96ca-5d1477f1e512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity batch: images=torch.Size([32, 3, 224, 224]), targets=torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "# Torch dataset with ELA conversion\n",
    "class ELAImageDataset(Dataset):\n",
    "    def __init__(self, csv_path, transform=None, ela_quality=90):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.image_paths = df['image'].tolist()\n",
    "        self.labels = df['class_label'].astype(int).tolist()\n",
    "        self.transform = transform\n",
    "        self.ela_quality = ela_quality\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.image_paths[idx]\n",
    "        label = float(self.labels[idx])\n",
    "        ela_im = convert_to_ela_image(path, self.ela_quality)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(ela_im)\n",
    "        else:\n",
    "            img = transforms.ToTensor()(ela_im)\n",
    "        # BCEWithLogitsLoss expects float targets of shape [N, 1]\n",
    "        y = torch.tensor([label], dtype=torch.float32)\n",
    "        return img, y\n",
    "\n",
    "# Transforms for ResNet50 input size 224x224\n",
    "# Train uses augmentations; Val is deterministic\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Build dataset from CSV built via build_image_list() using\n",
    "#   images/training/original/ (label 0)\n",
    "#   images/training/forged/   (label 1)\n",
    "csv_path = 'dataset.csv'\n",
    "full_dataset = ELAImageDataset(csv_path, transform=None, ela_quality=90)\n",
    "\n",
    "# Train-test split (aligned with original split settings)\n",
    "indices = np.arange(len(full_dataset))\n",
    "train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=5, shuffle=True, stratify=[full_dataset.labels[i] for i in indices])\n",
    "\n",
    "# Wrap subsets to apply different transforms\n",
    "class SubsetWithTransform(Dataset):\n",
    "    def __init__(self, base_dataset, indices, transform):\n",
    "        self.base = base_dataset\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def __getitem__(self, i):\n",
    "        img, y = self.base[self.indices[i]]\n",
    "        # base returns tensor if base has transform; here base has None, so img is PIL Image from ELA\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            # Defensive: if base was changed to yield tensor, just normalize pipeline\n",
    "            pil = transforms.ToPILImage()(img)\n",
    "            img = self.transform(pil)\n",
    "        else:\n",
    "            img = self.transform(img)\n",
    "        return img, y\n",
    "\n",
    "train_dataset = SubsetWithTransform(full_dataset, train_idx, train_transform)\n",
    "val_dataset = SubsetWithTransform(full_dataset, val_idx, val_transform)\n",
    "\n",
    "batch_size = 32\n",
    "# Use 0 workers on Windows to avoid potential DataLoader hang\n",
    "num_workers = 0\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=torch.cuda.is_available())\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Quick sanity check to ensure loaders iterate\n",
    "_sanity_images, _sanity_targets = next(iter(train_loader))\n",
    "print(f\"Sanity batch: images={_sanity_images.shape}, targets={_sanity_targets.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "164acba2-8e57-4966-b361-c3cfdb0c9133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained ResNet50 and modify head for binary classification\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "\n",
    "# Replace final layer for binary classification (logit)\n",
    "in_features = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(in_features, 1)\n",
    "\n",
    "# Phase 1: freeze all backbone layers\n",
    "for name, param in resnet.named_parameters():\n",
    "    param.requires_grad = False\n",
    "# Unfreeze only the final classification head\n",
    "for param in resnet.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "# Print summary for frozen model\n",
    "try:\n",
    "    summary(resnet, (3, 224, 224))\n",
    "except Exception as e:\n",
    "    print(resnet)\n",
    "\n",
    "# Loss and optimizer for phase 1\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()), lr=1e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfbf1b77-3277-453e-b7ac-c2d711e2471c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs_phase1 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     51\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m train_one_epoch(resnet, train_loader, criterion, optimizer, device)\n\u001b[1;32m---> 52\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphase1_train_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m     55\u001b[0m     history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphase1_val_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(val_loss)\n",
      "Cell \u001b[1;32mIn[12], line 37\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, loader, criterion, device)\u001b[0m\n\u001b[0;32m     35\u001b[0m         correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m==\u001b[39m targets)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     36\u001b[0m         total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrunning_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m, correct \u001b[38;5;241m/\u001b[39m total\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, targets in tqdm(loader, desc='Train', leave=False):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct += (preds == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(loader, desc='Eval', leave=False):\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "\n",
    "# Training configuration analogous to original (epochs/batch_size adjusted for ResNet)\n",
    "epochs_phase1 = 10\n",
    "history = {\n",
    "    'phase1_train_loss': [], 'phase1_val_loss': [],\n",
    "    'phase1_train_acc': [], 'phase1_val_acc': [],\n",
    "    'phase2_train_loss': [], 'phase2_val_loss': [],\n",
    "    'phase2_train_acc': [], 'phase2_val_acc': [],\n",
    "}\n",
    "\n",
    "best_val_acc_phase1 = 0.0\n",
    "for epoch in range(1, epochs_phase1 + 1):\n",
    "    train_loss, train_acc = train_one_epoch(resnet, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = evaluate(resnet, val_loader, criterion, device)\n",
    "\n",
    "    history['phase1_train_loss'].append(train_loss)\n",
    "    history['phase1_val_loss'].append(val_loss)\n",
    "    history['phase1_train_acc'].append(train_acc)\n",
    "    history['phase1_val_acc'].append(val_acc)\n",
    "\n",
    "    print(f\"[Phase 1] Epoch {epoch}/{epochs_phase1} - loss: {train_loss:.4f} acc: {train_acc:.4f} - val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc_phase1:\n",
    "        best_val_acc_phase1 = val_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ad54d2-37bc-4d7a-8f72-dca674bdcdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: unfreeze last few layers for fine-tuning\n",
    "# Limit to layer4 only to reduce overfitting risk\n",
    "\n",
    "# Re-freeze everything first (safety)\n",
    "for p in resnet.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Unfreeze only layer4\n",
    "for name, module in resnet.named_children():\n",
    "    if name == 'layer4':\n",
    "        for p in module.parameters():\n",
    "            p.requires_grad = True\n",
    "# Keep head trainable\n",
    "for p in resnet.fc.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Differential learning rates and weight decay\n",
    "backbone_params = [p for n, p in resnet.named_parameters() if p.requires_grad and not n.startswith('fc.')]\n",
    "head_params = [p for n, p in resnet.named_parameters() if p.requires_grad and n.startswith('fc.')]\n",
    "optimizer_ft = torch.optim.Adam([\n",
    "    {'params': backbone_params, 'lr': 1e-4, 'weight_decay': 1e-4},\n",
    "    {'params': head_params, 'lr': 3e-4, 'weight_decay': 1e-4},\n",
    "])\n",
    "\n",
    "# Print summary for fine-tuning configuration\n",
    "try:\n",
    "    summary(resnet, (3, 224, 224))\n",
    "except Exception as e:\n",
    "    print(resnet)\n",
    "\n",
    "# Early stopping on val loss\n",
    "epochs_phase2 = 10\n",
    "best_val = float('inf')\n",
    "patience = 3\n",
    "pat = 0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, epochs_phase2 + 1):\n",
    "    train_loss, train_acc = train_one_epoch(resnet, train_loader, criterion, optimizer_ft, device)\n",
    "    val_loss, val_acc = evaluate(resnet, val_loader, criterion, device)\n",
    "\n",
    "    history['phase2_train_loss'].append(train_loss)\n",
    "    history['phase2_val_loss'].append(val_loss)\n",
    "    history['phase2_train_acc'].append(train_acc)\n",
    "    history['phase2_val_acc'].append(val_acc)\n",
    "\n",
    "    print(f\"[Phase 2] Epoch {epoch}/{epochs_phase2} - loss: {train_loss:.4f} acc: {train_acc:.4f} - val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        pat = 0\n",
    "        best_state = {k: v.cpu().clone() for k, v in resnet.state_dict().items()}\n",
    "    else:\n",
    "        pat += 1\n",
    "        if pat >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Restore best weights if early stopped\n",
    "if best_state is not None:\n",
    "    resnet.load_state_dict(best_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af58b3b-613d-469c-adbd-df47d8235a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fine-tuned model\n",
    "os.makedirs('model', exist_ok=True)\n",
    "torch.save(resnet.state_dict(), os.path.join('model', 'image_forgery_finetuned.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ee07a3-5946-4836-88de-f87343cca55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: evaluate on external test set if provided\n",
    "if test_loader is not None:\n",
    "    test_loss, test_acc = evaluate(resnet, test_loader, criterion, device)\n",
    "    print(f\"[Test] loss: {test_loss:.4f} acc: {test_acc:.4f}\")\n",
    "    # Test confusion matrix\n",
    "    resnet.eval()\n",
    "    all_preds, all_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, targets in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = resnet(images)\n",
    "            preds = (torch.sigmoid(outputs).cpu().numpy() > 0.5).astype(int).flatten().tolist()\n",
    "            all_preds.extend(preds)\n",
    "            all_true.extend(targets.numpy().astype(int).flatten().tolist())\n",
    "    cm_test = confusion_matrix(all_true, all_preds)\n",
    "    plot_confusion_matrix(cm_test, classes=range(2), title='Confusion matrix (Test)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd2eb73-8b9e-4115-b4ad-f02678597687",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2, figsize=(10,8))\n",
    "# Phase 1\n",
    "ax[0,0].plot(history['phase1_train_loss'], color='b', label='Training loss (P1)')\n",
    "ax[0,0].plot(history['phase1_val_loss'], color='r', label='Validation loss (P1)')\n",
    "ax[0,0].legend(loc='best', shadow=True)\n",
    "ax[0,0].set_title('Phase 1 Loss')\n",
    "\n",
    "ax[1,0].plot(history['phase1_train_acc'], color='b', label='Training acc (P1)')\n",
    "ax[1,0].plot(history['phase1_val_acc'], color='r', label='Validation acc (P1)')\n",
    "ax[1,0].legend(loc='best', shadow=True)\n",
    "ax[1,0].set_title('Phase 1 Accuracy')\n",
    "\n",
    "# Phase 2\n",
    "ax[0,1].plot(history['phase2_train_loss'], color='b', label='Training loss (P2)')\n",
    "ax[0,1].plot(history['phase2_val_loss'], color='r', label='Validation loss (P2)')\n",
    "ax[0,1].legend(loc='best', shadow=True)\n",
    "ax[0,1].set_title('Phase 2 Loss')\n",
    "\n",
    "ax[1,1].plot(history['phase2_train_acc'], color='b', label='Training acc (P2)')\n",
    "ax[1,1].plot(history['phase2_val_acc'], color='r', label='Validation acc (P2)')\n",
    "ax[1,1].legend(loc='best', shadow=True)\n",
    "ax[1,1].set_title('Phase 2 Accuracy')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5762ef0e-a26d-41f5-8f8f-0729584b1b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Predict validation set for confusion matrix\n",
    "resnet.eval()\n",
    "all_preds = []\n",
    "all_true = []\n",
    "with torch.no_grad():\n",
    "    for images, targets in val_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = resnet(images)\n",
    "        preds = (torch.sigmoid(outputs).cpu().numpy() > 0.5).astype(int).flatten().tolist()\n",
    "        all_preds.extend(preds)\n",
    "        all_true.extend(targets.numpy().astype(int).flatten().tolist())\n",
    "\n",
    "confusion_mtx = confusion_matrix(all_true, all_preds)\n",
    "plot_confusion_matrix(confusion_mtx, classes=range(2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch (cu121)",
   "language": "python",
   "name": "pytorch-cu121"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
